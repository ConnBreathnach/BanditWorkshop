{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/conn/anaconda3/envs/BanditWorkshop2/lib/python3.10/site-packages (1.23.1)\r\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.\r\n",
      "You should consider upgrading via the '/home/conn/anaconda3/envs/BanditWorkshop2/bin/python -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bandit Environment\n",
    "### Call Bandit(min, max) to create a bandit with range or rewards from min to max\n",
    "### Call Bandit.sample() to sample from this distribution\n",
    "### sample() will return a random value in the reward range"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Bandit instance, used as environment\n",
    "class Bandit:\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.times_chosen = 1\n",
    "        self.dist_range = sorted(np.random.randint(min_val, max_val+1, 2))\n",
    "        self.true_reward = (self.dist_range[0] + self.dist_range[1]) / 2 # Mean of the reward distribution\n",
    "        print(self.dist_range)\n",
    "\n",
    "    def sample(self):\n",
    "        self.times_chosen += 1\n",
    "        # Return a random value from the reward distribution (uniform between dist_range[0] and dist_range[1])\n",
    "        #TUTORIAL: how can we sample this random value? (hint. np.random.uniform function might be a good start)\n",
    "        return np.random.uniform(self.dist_range[0], self.dist_range[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Agent needs to:\n",
    "###    a) Keep track of how \"good\" each bandit has been in terms of reward\n",
    "###    b) Keep track of regret"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_bandits=10, dist_min=1, dist_max=10):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.bandits = []\n",
    "        self.rewards_avg = [] # What we assume the reward to be, based on what we have seen. Will constantly update as we sample more\n",
    "        self.best_reward = -1\n",
    "        self.regret = 0\n",
    "        self.time = 1\n",
    "        #Create bandits for the agent to interact with\n",
    "        for i in range(self.num_bandits):\n",
    "            self.bandits.append(Bandit(dist_min, dist_max))\n",
    "            self.rewards_avg.append(1)\n",
    "            #We want to keep track of what's the best possible reward (on average)\n",
    "            #TUTORIAL: how can we keep track of this best reward\n",
    "            if self.bandits[i].true_reward > self.best_reward:\n",
    "                self.best_reward = self.bandits[i].true_reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Greedy Agent\n",
    "## The Greedy Agent chooses the best action at every time (pure exploitation). Is this what we want? What are the advantages and disadvantages of this?\n",
    "## action = $arg max(\\text{Bandit rewards})$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class GreedyAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        #Choose best action\n",
    "        #TUTORIAL: how do we choose the index of the highest reward bandit? (hint: we have some variables in the Agent superclass that keep track of every bandit's presumed values)\n",
    "        bandit_idx = np.argmax(self.rewards_avg)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"GREEDY: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimistic Greedy Agent\n",
    "## This agent works the same as the greedy agent, except the expected reward is set high for all bandits initially, so that the mean value is brought \"down\", which means that one agent shouldn't ever be always selected after the first run"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class OptimisticGreedyAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        for i in range(len(self.rewards_avg)):\n",
    "            #TUTORIAL: What value can we set the rewards_avg to?\n",
    "            self.rewards_avg[i] = 10\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        #Choose best action\n",
    "        #TUTORIAL (same as greedy agent)\n",
    "        bandit_idx = np.argmax(self.rewards_avg)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"OPTIMISTIC GREEDY: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# e-Greedy Agent\n",
    "## The Epsilon Greedy Agent chooses the best action most of the time (exploitation) but every so often chooses a random action (exploration). Why might this work? What are the advantages and disadvantages of this?\n",
    "## action = USUALLY $argmax(\\text{Bandit rewards})$ but SOMETIMES $random(Bandit)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, epsilon = 0.9):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        if np.random.uniform() > self.epsilon: # Choose random action\n",
    "            bandit_idx = np.random.randint(0, self.num_bandits)\n",
    "        else: # Choose best action\n",
    "            bandit_idx = np.argmax(self.rewards_avg)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"EPSILON GREEDY: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UCB Agent\n",
    "## The Upper Confidence Bound Agent chooses the agent that has the maximum best reward + least confidence. This means that actions chosen long ago are more uncertain on their reward than actions chosen recently. We assign a \"confidence\" to these actions (based on our confidence on how good/bad they actually were), and then choose our action based on the reward we think they give, plus the confidence\n",
    "## action = $arg max [\\text{bandit reward} + confidence * \\sqrt{\\frac{log(t)}{ N_t a}} ]$\n",
    "### Where $t$ is total time and $N_t$ is number of times the action has been selected"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class UCBAgent(Agent):\n",
    "    def __init__(self, confidence=2):\n",
    "        super().__init__()\n",
    "        self.confidence = confidence\n",
    "        self.times_since_last_choice = [1 for i in range(self.num_bandits)]\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        confidences = []\n",
    "        for bandit_idx in range(len(self.bandits)):\n",
    "            est_reward = self.rewards_avg[bandit_idx]\n",
    "            #TUTORIAL: Calculate the confidence value. Each agent has a .time value for how many actions have been taken, and each bandit has a .times_chosen\n",
    "            confidence_val = self.confidence * np.sqrt(np.log(self.time)/self.bandits[bandit_idx].times_chosen)\n",
    "            confidences.append(est_reward + confidence_val)\n",
    "        self.time += 1\n",
    "        bandit_idx = np.argmax(confidences)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "        #Calculate new reward average\n",
    "        old_rewards_avg = self.rewards_avg[bandit_idx]\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"UCB: Bandit {} was chosen ({}, {}), with reward {}, expected_reward {}, confidence {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, old_rewards_avg, confidences[bandit_idx], self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9]\n",
      "[2, 8]\n",
      "[4, 8]\n",
      "[9, 9]\n",
      "[3, 7]\n",
      "[2, 9]\n",
      "[3, 8]\n",
      "[4, 8]\n",
      "[8, 10]\n",
      "[1, 4]\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.049650481247589, expected_reward 1, confidence 1.0, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.876798105561187, expected_reward 4.5248252406237945, confidence 5.702235263139269, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.957896702951283, expected_reward 5.642149528936258, confidence 6.852445519547982, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.428815405468189, expected_reward 6.471086322440014, confidence 7.648496344955489, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.104719396690154, expected_reward 6.66263213904565, confidence 7.7973348886445395, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.7924414469997, expected_reward 6.736313348653067, confidence 7.829248073519426, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.180939280805518, expected_reward 7.0300459341311585, confidence 8.084535695391569, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.0983182654303345, expected_reward 7.173907602465453, confidence 8.193574592634262, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.030875664524782, expected_reward 7.054397676128218, confidence 8.042600214373225, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.883737876281403, expected_reward 7.152045474967875, confidence 8.11175065740549, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.1058099881645465, expected_reward 7.2185629659963775, confidence 8.152351977166045, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.876128363459369, expected_reward 7.125833551177059, confidence 8.035944652414704, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.887942030627359, expected_reward 7.183548536737236, confidence 8.071926376910685, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.633603418755737, expected_reward 7.30529092915796, confidence 8.173632094766313, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.89033313046244, expected_reward 7.327178428464478, confidence 8.176970591463128, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.302196024258258, expected_reward 7.4248755973393505, confidence 8.257430208497048, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.828973736198925, expected_reward 7.417659151863993, confidence 8.234138443637875, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.389486880427613, expected_reward 7.440509962104822, confidence 8.241949189425295, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.247109783053507, expected_reward 7.385192957806022, confidence 8.17251874193279, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.778144178819542, expected_reward 7.428288799068396, confidence 8.202334311109386, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.811135541272606, expected_reward 7.4925676266755925, confidence 8.254085213968784, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.046905260661443, expected_reward 7.5525025318845485, confidence 8.302174272576714, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.139583657615292, expected_reward 7.5305200418313705, confidence 8.268966615322453, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.162623414589884, expected_reward 7.555897692489034, confidence 8.28368587485484, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.446268018419368, expected_reward 7.540166721373068, confidence 8.257815752570709, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.413260143295574, expected_reward 7.536555232797927, confidence 8.244542253324486, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.058124183062754, expected_reward 7.494951710964505, confidence 8.19371642694331, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.813754346482723, expected_reward 7.479350727825157, confidence 8.169299432975993, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.075910739177872, expected_reward 7.4563991284685205, confidence 8.137908187278097, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.302748033327793, expected_reward 7.4103828488255, confidence 8.08380172606434, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.666699570892355, expected_reward 7.439168822519122, confidence 8.104822728959528, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.660824744557086, expected_reward 7.446279158405785, confidence 8.104471370339326, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.999538730833333, expected_reward 7.422477509501279, confidence 8.073491409782658, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.19126931567061, expected_reward 7.4394498983639865, confidence 8.083550778717747, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.1716052704615425, expected_reward 7.403787596001319, confidence 8.041224253828313, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.457583240559845, expected_reward 7.397338086958547, confidence 8.02834424456683, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.777475024229811, expected_reward 7.425993361380205, confidence 8.050788931202563, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.823266104065201, expected_reward 7.408927089349932, confidence 8.0277193051114, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.516866056169553, expected_reward 7.4451921922913495, confidence 8.05817662310795, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.00546031654437, expected_reward 7.421984038888304, confidence 8.029345500796609, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.685481357730622, expected_reward 7.436215167611623, confidence 8.038128544954917, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.229194776625278, expected_reward 7.418340553090647, confidence 8.014971540447661, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.607957505355762, expected_reward 7.437197628056568, confidence 8.028703401933402, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.015395231708064, expected_reward 7.441078534358823, confidence 8.027608362625992, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.453209149989256, expected_reward 7.45384112763325, confidence 8.035536923638942, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.806504250181336, expected_reward 7.475566519423599, confidence 8.052563346819676, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.945701304761676, expected_reward 7.503884343482274, confidence 8.076310877035489, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.13020267972915, expected_reward 7.513088863508927, confidence 8.081067810515174, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.378039212856618, expected_reward 7.5256830230236265, confidence 8.089331509380413, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.638660776501261, expected_reward 7.502730146820286, confidence 8.062160071327593, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.152678690843367, expected_reward 7.525003296421874, confidence 8.080321656462297, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.250062172914174, expected_reward 7.4986124386222865, confidence 8.049921630007905, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.300650967221196, expected_reward 7.475054886439114, confidence 8.022452979896622, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.353910812717194, expected_reward 7.453306665712857, confidence 7.996887662203262, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.410025611442757, expected_reward 7.433317650203844, confidence 7.973171717032512, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.28530111454685, expected_reward 7.432901720940253, confidence 7.969115410402575, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.115823960725775, expected_reward 7.430312236617561, confidence 7.9629686887406805, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.379122207004038, expected_reward 7.40764864565391, confidence 7.9368277764323985, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.045282526143698, expected_reward 7.4071651466937425, confidence 7.932943823073815, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.011884655719096, expected_reward 7.417800436351242, confidence 7.940252639091493, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.0633184760667955, expected_reward 7.3947526366687475, confidence 7.913949612093348, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.86184608528781, expected_reward 7.389406924400975, confidence 7.905417325961872, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.633941912626522, expected_reward 7.396905958700765, confidence 7.909795979177109, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.883087778161586, expected_reward 7.416234645480856, confidence 7.92606814056526, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.522474981950094, expected_reward 7.438801616752867, confidence 7.945640220703983, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.410196615141031, expected_reward 7.424917879861916, confidence 7.928821113840151, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.469240340479605, expected_reward 7.409772786358619, confidence 7.91079816000577, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.95056348356205, expected_reward 7.4253531915662805, confidence 7.923556298339607, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.524134738436851, expected_reward 7.432964934928537, confidence 7.928399541654662, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.473890685729932, expected_reward 7.448553074978657, confidence 7.941271206052129, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.463805148414489, expected_reward 7.434825435693463, confidence 7.924877452308274, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.025860342632353, expected_reward 7.421339042814588, confidence 7.908773717583996, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.4855748940743565, expected_reward 7.415921526373735, confidence 7.900786113663971, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.680061373057168, expected_reward 7.41686278809942, confidence 7.899203090375961, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.856237440112419, expected_reward 7.4337054358988555, confidence 7.913565866359564, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.45020446560878, expected_reward 7.426107172796402, confidence 7.903530814543729, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.274047045484151, expected_reward 7.439407137638121, confidence 7.914435799622516, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.784593140710747, expected_reward 7.4501076492771725, confidence 7.9227819192255815, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.801585704398137, expected_reward 7.466999870687724, confidence 7.9373591652144695, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.509117948652369, expected_reward 7.483682193609104, confidence 7.951764805691303, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.724695083899586, expected_reward 7.471650536263959, confidence 7.937493680249828, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.040261686341656, expected_reward 7.462541323430248, confidence 7.926181177736039, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.746695194052123, expected_reward 7.469501809730385, confidence 7.9309735573701685, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.842689606514756, expected_reward 7.460896969067549, confidence 7.9202348361495, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.918245330622483, expected_reward 7.465388647155163, confidence 7.922625939468353, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.739002186331212, expected_reward 7.482282329521063, confidence 7.937451467327869, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.60000216896897, expected_reward 7.485233132472903, confidence 7.938365683614022, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.18180328381416, expected_reward 7.497900962433087, confidence 7.949027673844647, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.214734035922703, expected_reward 7.50558525817894, confidence 7.954736085914321, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.1746888861991565, expected_reward 7.51346468904276, confidence 7.960668826885387, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.219034409331442, expected_reward 7.49875286703349, confidence 7.944038773781046, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.722005616104163, expected_reward 7.49571244901499, confidence 7.939107874510064, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.459435470100104, expected_reward 7.508898397048207, confidence 7.950430407025455, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.887236278530628, expected_reward 7.519010493570035, confidence 7.958705493385309, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.112621024559144, expected_reward 7.512360238674884, confidence 7.950243995977545, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.549735461666144, expected_reward 7.497779621861178, confidence 7.933877288266873, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.496486645700319, expected_reward 7.5086245274261785, confidence 7.942960659243693, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.837986014742825, expected_reward 7.498296589857547, confidence 7.930895167919999, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.549392233315243, expected_reward 7.511828806270529, confidence 7.942713254918012, and regret 1.5, best reward 9.0\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.509795175812818, expected_reward 7.512204440540976, confidence 7.941397645798845, and regret 1.5, best reward 9.0\n",
      "Final regret: 150.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = UCBAgent()\n",
    "    for i in range(100):\n",
    "        agent.choose_bandit()\n",
    "    print(\"Final regret: {}\".format(agent.regret))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}