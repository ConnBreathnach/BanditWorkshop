{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/conn/miniconda3/envs/BanditWorkshop/lib/python3.10/site-packages (1.23.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bandit Environment\n",
    "### Call Bandit(min, max) to create a bandit with range or rewards from min to max\n",
    "### Call Bandit.sample() to sample from this distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "#Bandit instance, used as environment\n",
    "class Bandit:\n",
    "    def __init__(self, min_val, max_val):\n",
    "        self.times_chosen = 1\n",
    "        self.dist_range = sorted(np.random.randint(min_val, max_val+1, 2))\n",
    "        self.true_reward = (self.dist_range[0] + self.dist_range[1]) / 2\n",
    "        print(self.dist_range)\n",
    "\n",
    "    def sample(self):\n",
    "        self.times_chosen += 1\n",
    "        return np.random.uniform(self.dist_range[0], self.dist_range[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Agent needs to:\n",
    "###    a) Keep track of how \"good\" each bandit has been in terms of reward\n",
    "###    b) Keep track of regret"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_bandits=10, dist_min=1, dist_max=10):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.bandits = []\n",
    "        self.rewards_avg = []\n",
    "        self.best_reward = -1\n",
    "        self.regret = 0\n",
    "        self.time = 1\n",
    "        for i in range(self.num_bandits):\n",
    "            self.bandits.append(Bandit(dist_min, dist_max))\n",
    "            self.rewards_avg.append(1)\n",
    "            if self.bandits[i].true_reward > self.best_reward:\n",
    "                self.best_reward = self.bandits[i].true_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Greedy Agent\n",
    "## The Greedy Agent chooses the best action at every time (pure exploitation). Is this what we want? What are the advantages and disadvantages of this?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class GreedyAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        #Choose best action\n",
    "        bandit_idx = np.argmax(self.rewards_avg)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"GREEDY: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# e-Greedy Agent\n",
    "## The Epsilon Greedy Agent chooses the best action most of the time (exploitation) but every so often chooses a random action (exploration). Why might this work? What are the advantages and disadvantages of this?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(Agent):\n",
    "    def __init__(self, epsilon = 0.9):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        if np.random.uniform() > self.epsilon: # Choose random action\n",
    "            bandit_idx = np.random.randint(0, self.num_bandits)\n",
    "        else: # Choose best action\n",
    "            bandit_idx = np.argmax(self.rewards_avg)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"EPSILON GREEDY: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UCB Agent\n",
    "## The Upper Confidence Bound Agent chooses the agent that has the maximum best reward + least confidence. This means that actions chosen long ago are more uncertain on their reward than actions chosen recently. We assign a \"confidence\" to these actions (based on our confidence on how good/bad they actually were), and then choose our action based on the reward we think they give, plus the confidence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class UCBAgent(Agent):\n",
    "    def __init__(self, confidence=2):\n",
    "        super().__init__()\n",
    "        self.confidence = confidence\n",
    "        self.times_since_last_choice = [1 for i in range(self.num_bandits)]\n",
    "\n",
    "    def choose_bandit(self):\n",
    "        confidences = []\n",
    "        for bandit_idx in range(len(self.bandits)):\n",
    "            est_reward = self.rewards_avg[bandit_idx]\n",
    "            confidence_val = self.confidence * np.sqrt(np.log(self.time)/self.bandits[bandit_idx].times_chosen)\n",
    "            confidences.append(est_reward + confidence_val)\n",
    "        self.time += 1\n",
    "        bandit_idx = np.argmax(confidences)\n",
    "        reward = self.bandits[bandit_idx].sample()\n",
    "        #Calculate new reward average\n",
    "        new_reward_avg = ((self.rewards_avg[bandit_idx] * (self.bandits[bandit_idx].times_chosen-1)) + reward) / self.bandits[bandit_idx].times_chosen\n",
    "        self.rewards_avg[bandit_idx] = new_reward_avg\n",
    "        #Calculate regret\n",
    "        self.regret += (self.best_reward - self.bandits[bandit_idx].true_reward)\n",
    "        print(\"UCB: Bandit {} was chosen ({}, {}), with reward {}, and regret {}, best reward {}\".format(bandit_idx, self.bandits[bandit_idx].dist_range[0], self.bandits[bandit_idx].dist_range[1], reward, self.best_reward - self.bandits[bandit_idx].true_reward, self.best_reward))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9]\n",
      "[3, 4]\n",
      "[1, 2]\n",
      "[2, 10]\n",
      "[3, 9]\n",
      "[4, 9]\n",
      "[6, 9]\n",
      "[1, 2]\n",
      "[2, 5]\n",
      "[5, 9]\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.363421804180451, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.902138847560177, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.943536455285491, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.818148931700662, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.56132505661118, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.407015667720913, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.633426286691158, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.4082958493890985, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.335732610216187, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.26631354100471, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.521984161935901, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.821986340546864, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.8986857766068095, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.38920981710239, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.775300783531664, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.89531048338327, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.3137066547052, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.032218415988286, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.033641027840337, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.189079900001712, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.195760615753743, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.357355331884621, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.311346621610295, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.743815779565211, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.238165214397467, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.005966869129129, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.08184517203371, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.446780677376601, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.9012161892733985, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.591221261561707, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.831811388358812, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.875368753687141, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.622663310439453, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.432168743660906, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.9745297711882195, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.345540576493049, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.48539125184478, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.300868186624982, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.6416992939262345, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.0366619458602635, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.68554771242778, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.399139820204138, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.959437456643876, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.906911923748126, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.012460764479199, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.46249846242793, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.060103275905375, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.633129998594606, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.14223908954397, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.008619471824256, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.631328989907243, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.714190602343152, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.922958875121676, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.871228203339732, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.635958348390975, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.827149124806532, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.126019769294473, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.6784302176735135, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.966028478157202, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.973550349639643, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.808710790576352, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.298242695454755, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.484766144547184, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.553102659509339, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.542857610731595, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.10771912689457, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.928893531042139, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.694076156102902, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.563350811681697, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.6924521983800895, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.597366791254501, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.158084399190908, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.022127136667521, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.046823033306682, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.41635235390703, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.914338247587474, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.907454594990398, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.494520140603806, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.286751630348963, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.226876394010525, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.22523899494756, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.011930238000099, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.054029309388605, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.166103275497003, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.593066257361181, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.562163979390358, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.5934397693897875, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.674621989564969, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.0257663957886525, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.925001174404471, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.5961066395774, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.936890287170912, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.1389985200020085, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.809654216528017, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.0205372923599345, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.133785763302086, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.543160710897329, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 8.239420419785805, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 7.219305130064316, and regret 0.0, best reward 7.5\n",
      "UCB: Bandit 0 was chosen (6, 9), with reward 6.4153618183747545, and regret 0.0, best reward 7.5\n",
      "Final regret: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = UCBAgent()\n",
    "    for i in range(100):\n",
    "        agent.choose_bandit()\n",
    "    print(\"Final regret: {}\".format(agent.regret))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
